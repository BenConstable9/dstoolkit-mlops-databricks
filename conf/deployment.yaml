custom:

  # Cluster configs for each environment
  default-cluster-spec: &default-cluster-spec
    spark_version: '10.4.x-scala2.12'
    node_type_id: 'Standard_D3_v2' # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    driver_node_type_id: 'Standard_D3_v2'  # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
#    driver_instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'
#    instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *default-cluster-spec


build:
  no_build: true
environments:
  default:
    workflows:
      - name: "e2e-mlops-Development"
        <<: *dev-cluster-config
        spark_python_task:
          python_file: "file://Data_Scientist/featureStoreTaxiMLOps.py"
          parameters: [ '--env', 'file:fuse://conf/job_params/sandbox.yaml', '{"env": "dev"}']
        #existing_cluster_id: 1128-145618-6i6i1ekh
        #libraries: [
        #  whl: "file://.dbx/conf/pyWheel_1-0.0.1-py3-none-any.whl"
        #  ]
  Sandbox:
    workflows:
      - name: "e2e-mlops-Sandbox"
        <<: *dev-cluster-config
        spark_python_task:
          python_file: "file://Data_Scientist/featureStoreTaxiMLOps.py"
          parameters: [ '--env', 'file:fuse://conf/job_params/sandbox.yaml', '{"env": "dev"}']
        #existing_cluster_id: 1128-145618-6i6i1ekh
        #libraries: [
        #  whl: "file://.dbx/conf/pyWheel_1-0.0.1-py3-none-any.whl"
        #  ]
  Development:
    workflows:
      - name: "e2e-MLOps-Development"
        #<<: *dev-cluster-config
        spark_python_task:
          python_file: "file://Data_Engineer/ETL.py"
          parameters: [ '--env', 'file:fuse://conf/job_params/sandbox.yaml', '{"env": "dev"}']
        existing_cluster_id: 1204-231501-b5749ycc
        #libraries: [
        #  whl: "file://.dbx/conf/pyWheel_1-0.0.1-py3-none-any.whl"
        #  ]
  UAT:
    workflows:
      - name: "e2e-MLOps-UAT"
        <<: *dev-cluster-config
        spark_python_task:
          python_file: "file://Data_Scientist/featureStoreTaxiMLOps.py"
          parameters: [ '--env', 'file:fuse://conf/job_params/sandbox.yaml', '{"env": "dev"}']
        #existing_cluster_id: 1128-145618-6i6i1ekh
        #libraries: [
        #  whl: "file://.dbx/conf/pyWheel_1-0.0.1-py3-none-any.whl"
        #  ]
  Production:
    workflows:
      - name: "e2e-mlops-Production"
        <<: *dev-cluster-config
        spark_python_task:
          python_file: "file://Data_Scientist/featureStoreTaxiMLOps.py"
          parameters: [ '--env', 'file:fuse://conf/job_params/sandbox.yaml', '{"env": "dev"}']
        #existing_cluster_id: 1128-145618-6i6i1ekh
        #libraries: [
        #  whl: "file://.dbx/conf/pyWheel_1-0.0.1-py3-none-any.whl"
        #  ]
        

